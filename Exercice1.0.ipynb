{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigdata: Scala + Spark\n",
    "_<font color=gray>Author: Elkin Dario Ortiz Landecho </font>_\n",
    "_<font color=gray>Date: 2020-07-22 </font>_\n",
    "***\n",
    "<br>\n",
    "<br>\n",
    "Actualmente, el volumen de información que se maneja en diferentes sectores demanda una organización de datos:\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"pictures/BigData_diagram.png\"  title=\"Bigdata Diagram\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Definición**: Conjunto de Datos caracterizado por su volumen, velocidad y variedad (**VVV**).\n",
    "<br>\n",
    "\n",
    "**Beneficios**:\n",
    "- Análisis de Datos\n",
    "- ETL(Extracción, transfomación y carga)**\n",
    "- Streaming\n",
    "- Procesamiento\n",
    "- Machine learnig\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## ETL\n",
    "\n",
    "Nos perminte la integración de datos. En Bigdata podemos las podemos construir: \n",
    "    1. Orquestador\n",
    "    2. Scala\n",
    "    3. Spark\n",
    "    4. Herramientas de integración Continua.\n",
    "\n",
    "<img src=\"pictures/Tools.png\"  title=\"Tools\">\n",
    "\n",
    "### 1. Orquestador\n",
    "Nos permite la contrucción y control del flujo de trabajo. Actualmente existen varios automatizadores de estos flujos que se puede usar según las necesidades Ej: \n",
    "\n",
    "- Apache Airflow\n",
    "- Apache kafka\n",
    "- AWS Step Functions\n",
    "\n",
    "### 2. Scala\n",
    "Leguaje que aprovecha la combinación de la programción orientada a objetos y la programación funcional. Similar a Java. Su demanda ha aumentado en los últimos años. A continuación realizaremos una pequeña introducción:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-54QUHHK:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1595380483655)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "a: Int = 1\r\n",
       "b: Int = 2\r\n",
       "c: Int = 3\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a: Int = 1\n",
    "val b = 2 //inmutable\n",
    "var c = 3 //mutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c: Int = 5\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "26: error: reassignment to val\r",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: reassignment to val\r",
      "       b = 3\r",
      "         ^",
      ""
     ]
    }
   ],
   "source": [
    "b = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d: String = hola mundo\r\n",
       "e: Char = e\r\n",
       "f: Double = 123.4\r\n",
       "g: Int = -5678\r\n",
       "h: Boolean = true\r\n",
       "i: Array[String] = Array(x, y, z)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = \"hola mundo\"\n",
    "val e = 'e'\n",
    "val f = 123.4\n",
    "val g = -5678\n",
    "val h = true\n",
    "val i = Array(\"x\",\"y\", \"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clases y Objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Point\r\n",
       "defined object Demo\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Point(val xc: Int, val yc: Int) {\n",
    "   var xf: Int = xc\n",
    "   var yf: Int = yc\n",
    "   \n",
    "   def move() {\n",
    "      println(\"Point x location : \" + xf)\n",
    "      println(\"Point y location : \" + yf) \n",
    "   }\n",
    "}\n",
    "\n",
    "object Demo {\n",
    "   def main() {\n",
    "      val pt = new Point(10, 20)\n",
    "      pt.move();\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point x location : 10\n",
      "Point y location : 20\n"
     ]
    }
   ],
   "source": [
    "Demo.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operadores aritmeticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y: 30\n",
      "x - y: -10\n",
      "x * y: 200\n",
      "x / y: 0\n",
      "x % y: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 10\r\n",
       "y: Int = 20\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x = 10\n",
    "var y = 20\n",
    "println(\"x + y: \" + (x + y) )\n",
    "println(\"x - y: \" + (x - y) )\n",
    "println(\"x * y: \" + (x * y) )\n",
    "println(\"x / y: \" + (x / y) )\n",
    "println(\"x % y: \" + (x % y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operadores relacionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x es igual que y: false\n",
      "x es diferente que y: false\n",
      "x es mayor que y: true\n",
      "x es menor que y: false\n",
      "x es mayor o igual que y: true\n",
      "x es menor o igual que y: false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 50\r\n",
       "y: Int = 25\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x = 50\n",
    "var y = 25\n",
    "\n",
    "println(\"x es igual que y: \" + (x == y) )\n",
    "println(\"x es diferente que y: \" + (x == y) )\n",
    "println(\"x es mayor que y: \" + (x > y) )\n",
    "println(\"x es menor que y: \" + (x < y) )\n",
    "println(\"x es mayor o igual que y: \" + (x >= y) )\n",
    "println(\"x es menor o igual que y: \" + (x <= y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operadores lógicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x AND y: false\n",
      "x OR y: true\n",
      "NOT x : false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Boolean = true\r\n",
       "y: Boolean = false\r\n",
       "z: Boolean = false\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x = true\n",
    "var y = false\n",
    "var z = false\n",
    "\n",
    "println(\"x AND y: \" + (x && y) )\n",
    "println(\"x OR y: \" + (x || y) )\n",
    "println(\"NOT x : \" + (!x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = x + 2 = 3"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 3\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x = 1\n",
    "x += 2 \n",
    "print(\"x = x + 2 = \" + x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprobado\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Double = 2.95\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x = 2.95\n",
    "if( x >= 2.95 ) println(\"Aprobado\")\n",
    " else println(\"Reprobado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y: Int = 1\r\n",
       "res6: String = Uno\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var y = 1\n",
    "\n",
    "y match {\n",
    "   case 1 => \"Uno\"\n",
    "   case 2 => \"Dos\"\n",
    "   case 3 => \"Tres\"\n",
    "   case 4 => \"Cuatro\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de x: 1\n",
      "Valor de x: 2\n",
      "Valor de x: 3\n",
      "Valor de x: 4\n",
      "Valor de x: 5\n",
      "Valor de x: 6\n",
      "Valor de x: 7\n",
      "Valor de x: 8\n",
      "Valor de x: 9\n"
     ]
    }
   ],
   "source": [
    "for( x <- 1 until 10){\n",
    "    println( \"Valor de x: \" + x );\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de x: 1\n",
      "Valor de x: 2\n",
      "Valor de x: 3\n",
      "Valor de x: 4\n",
      "Valor de x: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "xList: List[Int] = List(1, 2, 3, 4, 5)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xList = List(1,2,3,4,5)\n",
    "\n",
    "for( x <- xList ){\n",
    "    println( \"Valor de x: \" + x )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.Spark\n",
    "\n",
    "Los dataframe son Dataset(cojunto de Datos) organizados por columnas estan disponibles para Java y Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emptyDf: org.apache.spark.sql.DataFrame = []\r\n",
       "namesDf: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// crear un Dataframe son inmutables\n",
    "val emptyDf = spark.emptyDataFrame\n",
    "val namesDf = Seq((\"Juan\", \"Perez\"),(\"Andres\" , \"Rueda\")).toDF(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      Juan|    Perez|\n",
      "|    Andres|    Rueda|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      Juan|    Perez|\n",
      "+----------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// ver data \n",
    "emptyDf.show()\n",
    "namesDf.show()\n",
    "namesDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// ver estructura\n",
    "emptyDf.printSchema()\n",
    "namesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] = Array(first_name, last_name)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ver columnas\n",
    "namesDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world_happiness: org.apache.spark.sql.DataFrame = [Country name: string, Regional indicator: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CSV a Dataframe\n",
    "val world_happiness = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\").load(\"data/2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "root\n",
      " |-- Country name: string (nullable = true)\n",
      " |-- Regional indicator: string (nullable = true)\n",
      " |-- Ladder score: string (nullable = true)\n",
      " |-- Standard error of ladder score: string (nullable = true)\n",
      " |-- upperwhisker: string (nullable = true)\n",
      " |-- lowerwhisker: string (nullable = true)\n",
      " |-- Logged GDP per capita: string (nullable = true)\n",
      " |-- Social support: string (nullable = true)\n",
      " |-- Healthy life expectancy: string (nullable = true)\n",
      " |-- Freedom to make life choices: string (nullable = true)\n",
      " |-- Generosity: string (nullable = true)\n",
      " |-- Perceptions of corruption: string (nullable = true)\n",
      " |-- Ladder score in Dystopia: string (nullable = true)\n",
      " |-- Explained by: Log GDP per capita: string (nullable = true)\n",
      " |-- Explained by: Social support: string (nullable = true)\n",
      " |-- Explained by: Healthy life expectancy: string (nullable = true)\n",
      " |-- Explained by: Freedom to make life choices: string (nullable = true)\n",
      " |-- Explained by: Generosity: string (nullable = true)\n",
      " |-- Explained by: Perceptions of corruption: string (nullable = true)\n",
      " |-- Dystopia + residual: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(world_happiness.count())\n",
    "world_happiness.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|  Country name|\n",
      "+--------------+\n",
      "|       Finland|\n",
      "|       Denmark|\n",
      "|   Switzerland|\n",
      "|       Iceland|\n",
      "|        Norway|\n",
      "|   Netherlands|\n",
      "|        Sweden|\n",
      "|   New Zealand|\n",
      "|       Austria|\n",
      "|    Luxembourg|\n",
      "|        Canada|\n",
      "|     Australia|\n",
      "|United Kingdom|\n",
      "|        Israel|\n",
      "|    Costa Rica|\n",
      "|       Ireland|\n",
      "|       Germany|\n",
      "| United States|\n",
      "|Czech Republic|\n",
      "|       Belgium|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------------+\n",
      "|Healthy life expectancy|\n",
      "+-----------------------+\n",
      "|            67.69958496|\n",
      "+-----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|Healthy life expectancy|\n",
      "+-----------------------+\n",
      "|            67.69958496|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\r\n",
       "expectancy_life: org.apache.spark.sql.DataFrame = [Country name: string, Regional indicator: string ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// filtrar y seleccionar en un dataframe\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "world_happiness.select($\"Country name\").show()\n",
    "val expectancy_life = world_happiness.select(\n",
    "    $\"Country name\",\n",
    "    $\"Regional indicator\",\n",
    "    $\"Healthy life expectancy\".cast(\"double\")\n",
    ")\n",
    "\n",
    "world_happiness.filter(\n",
    "    $\"Country name\" === \"Colombia\" \n",
    ").select(\n",
    "    $\"Healthy life expectancy\"\n",
    ").show()\n",
    "\n",
    "world_happiness.filter(\n",
    "    $\"Country name\" === \"Colombia\"\n",
    ").select(\n",
    "    $\"Healthy life expectancy\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country name: string (nullable = true)\n",
      " |-- Regional indicator: string (nullable = true)\n",
      " |-- Healthy life expectancy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expectancy_life.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+\n",
      "|Regional indicator                |count|\n",
      "+----------------------------------+-----+\n",
      "|South Asia                        |7    |\n",
      "|Middle East and North Africa      |17   |\n",
      "|North America and ANZ             |4    |\n",
      "|Sub-Saharan Africa                |39   |\n",
      "|East Asia                         |6    |\n",
      "|Commonwealth of Independent States|12   |\n",
      "|Latin America and Caribbean       |21   |\n",
      "|Western Europe                    |21   |\n",
      "|Central and Eastern Europe        |17   |\n",
      "|Southeast Asia                    |9    |\n",
      "+----------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// agrupaciones\n",
    "expectancy_life.groupBy($\"Regional indicator\").count().show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------+\n",
      "|Regional indicator                |max life expectancy|\n",
      "+----------------------------------+-------------------+\n",
      "|South Asia                        |70.59999847        |\n",
      "|Middle East and North Africa      |73.20025635        |\n",
      "|North America and ANZ             |73.60453796        |\n",
      "|Sub-Saharan Africa                |66.40434265        |\n",
      "|East Asia                         |76.77170563        |\n",
      "|Commonwealth of Independent States|66.75065613        |\n",
      "|Latin America and Caribbean       |71.29985046        |\n",
      "|Western Europe                    |74.40270996        |\n",
      "|Central and Eastern Europe        |71.1029892         |\n",
      "|Southeast Asia                    |76.80458069        |\n",
      "+----------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expectancy_life.groupBy(\n",
    "    $\"Regional indicator\"\n",
    ").agg(\n",
    "    max($\"Healthy life expectancy\").as(\"max life expectancy\")\n",
    ").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectancy_life_2020: org.apache.spark.sql.DataFrame = [Country name: string, Regional indicator: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//agregar columna\n",
    "val expectancy_life_2020 = expectancy_life.withColumn(\"year\",lit(\"2020\").cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------------------+----+\n",
      "|  Country name|  Regional indicator|Healthy life expectancy|year|\n",
      "+--------------+--------------------+-----------------------+----+\n",
      "|       Finland|      Western Europe|             71.9008255|2020|\n",
      "|       Denmark|      Western Europe|            72.40250397|2020|\n",
      "|   Switzerland|      Western Europe|            74.10244751|2020|\n",
      "|       Iceland|      Western Europe|                   73.0|2020|\n",
      "|        Norway|      Western Europe|            73.20078278|2020|\n",
      "|   Netherlands|      Western Europe|            72.30091858|2020|\n",
      "|        Sweden|      Western Europe|            72.60076904|2020|\n",
      "|   New Zealand|North America and...|            73.20262909|2020|\n",
      "|       Austria|      Western Europe|            73.00250244|2020|\n",
      "|    Luxembourg|      Western Europe|            72.59999847|2020|\n",
      "|        Canada|North America and...|            73.60160065|2020|\n",
      "|     Australia|North America and...|            73.60453796|2020|\n",
      "|United Kingdom|      Western Europe|            72.30160522|2020|\n",
      "|        Israel|Middle East and N...|            73.20025635|2020|\n",
      "|    Costa Rica|Latin America and...|            71.29985046|2020|\n",
      "|       Ireland|      Western Europe|            72.30078888|2020|\n",
      "|       Germany|      Western Europe|            72.20201874|2020|\n",
      "| United States|North America and...|            68.29949951|2020|\n",
      "|Czech Republic|Central and Easte...|            70.04793549|2020|\n",
      "|       Belgium|      Western Europe|            72.00164795|2020|\n",
      "+--------------+--------------------+-----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expectancy_life_2020.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citiesDf: org.apache.spark.sql.DataFrame = [Country name: string, City: string]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val citiesDf = Seq((\"Colombia\", \"Bogota\"),(\"Colombia\" , \"Medellin\")).toDF(\"Country name\", \"City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citiesAndCountries: org.apache.spark.sql.DataFrame = [country: string, regional: string ... 1 more field]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join pueden ser inner, left, right, outer, leftsemi, leftanti\n",
    "val citiesAndCountries = expectancy_life_2020.as(\"table1\").join(\n",
    "    citiesDf.as(\"table2\"), \n",
    "    $\"table1.Country name\" === $\"table2.Country name\",\n",
    "    \"inner\"\n",
    ").select(\n",
    "    $\"table1.Country name\".as(\"country\"),\n",
    "    $\"table1.Regional indicator\".as(\"regional\"),\n",
    "    $\"table2.City\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " path file:/C:/Users/Laptop/Desktop/BigData_Scala_Spark/CSV already exists.;\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/C:/Users/Laptop/Desktop/BigData_Scala_Spark/CSV already exists.;\r",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r",
      "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\r",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "//Escribir CSV, avro o parquet \n",
    "citiesAndCountries.write.mode(\"overwrite\").parquet(\"parquet\")\n",
    "citiesAndCountries.write.format(\"com.databricks.spark.avro\").mode(\"overwrite\").save(\"avro\")\n",
    "citiesAndCountries.write.option(\"header\", \"true\").csv(\"CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "\n",
    "La redistribución de cargas en diferentes nodos o diferentes maquinas. el uso de join, groupBy aumentan estas reparticiones. \n",
    "\n",
    "## Buenas prácticas\n",
    "\n",
    "- Validar el comportamiento de dataframes con el uso del comando _**explain()**_. **Spark 3 mejora esta funcionalidad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [Country name#34 AS country#206, Regional indicator#35 AS regional#207, City#185]\n",
      "+- *(1) BroadcastHashJoin [Country name#34], [Country name#184], Inner, BuildRight\n",
      "   :- *(1) Project [Country name#34, Regional indicator#35]\n",
      "   :  +- *(1) Filter isnotnull(Country name#34)\n",
      "   :     +- *(1) FileScan csv [Country name#34,Regional indicator#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Laptop/Desktop/BigData_Scala_Spark/data/2020.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Country name)], ReadSchema: struct<Country name:string,Regional indicator:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- LocalTableScan [Country name#184, City#185]\n"
     ]
    }
   ],
   "source": [
    "citiesAndCountries.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conocer las herramientas con las que trabajas y validar en lo posible las configuraciones de spark:\n",
    "http://spark.apache.org/docs/latest/sql-performance-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@7623831c\r\n",
       "res20: String = 10485760\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
    "sqlContext.getConf(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evitar el uso del _**count()**_ a menos que sea necesario retornar el número de filas.\n",
    "- Cuando se requiere agrupar analizar cuál función utilizar: _**groupBy, groupByKey, reduceByKey, TreeReduce/TreeAggregate**_\n",
    "- Al realizar Join validar data para utilizar estrategias como: _**broadcast, Sort Merge, Shuffle Hash o Cartesian**_. **Spark 3 a optimizado el uso de estas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citiesAndCountries: org.apache.spark.sql.DataFrame = [country: string, regional: string ... 1 more field]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val citiesAndCountries = expectancy_life_2020.as(\"table1\").join(\n",
    "    broadcast(citiesDf).as(\"table2\"), \n",
    "    $\"table1.Country name\" === $\"table2.Country name\",\n",
    "    \"inner\"\n",
    ").select(\n",
    "    $\"table1.Country name\".as(\"country\"),\n",
    "    $\"table1.Regional indicator\".as(\"regional\"),\n",
    "    $\"table2.City\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dependiendo de la complejidad de transformaciones y recursos puede guardar algunos datos en memoria.\n",
    "https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/\n",
    "- Serializar es importante en aplicaciones distribuidas.\n",
    "- Evitar el uso del crossjoin.\n",
    "- comunicación de equipo (data engineer, arquitectos, devops, cientificos, gobierno de datos )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
